# NERSOTA-SLNER-ELECTRA

2022 TWIGFARM SOL Project VOL.03
"í•œêµ­ì–´ êµ¬ì–´ì²´ NER ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì—°êµ¬ í”„ë¡œì íŠ¸"

# Data

## 1. Pretraining Corpus

[AIHub] ë°©ì†¡ ì½˜í…ì¸  ëŒ€ë³¸ ìš”ì•½ ë°ì´í„°] <br>
[AIHub] ì¼ìƒìƒí™œ ë° êµ¬ì–´ì²´ í•œ-ì¤‘, í•œ-ì¼ ë²ˆì—­ ë³‘ë ¬ ë§ë­‰ì¹˜ ë°ì´í„° <br>
[AIHub] ë‹¤êµ­ì–´ êµ¬ì–´ì²´ ë²ˆì—­ ë³‘ë ¬ ë§ë­‰ì¹˜ ë°ì´í„° <br>
[ëª¨ë‘ì˜ ë§ë­‰ì¹˜] êµ¬ì–´ì²´ ë§ë­‰ì¹˜ <br>
[ëª¨ë‘ì˜ ë§ë­‰ì¹˜] ì¼ìƒ ëŒ€í™” ë§ë­‰ì¹˜ 2020 <br>

## 2. Finetuning Corpus

[AIHub] ë°©ì†¡ ì½˜í…ì¸  í•œ-ì¤‘, í•œ-ì¼ ë²ˆì—­ ë³‘ë ¬ ë§ë­‰ì¹˜ ë°ì´í„° <br>
[AIHub] ì¼ìƒìƒí™œ ë° êµ¬ì–´ì²´ í•œ-ì˜ ë²ˆì—­ ë³‘ë ¬ ë§ë­‰ì¹˜ ë°ì´í„° <br>
[ëª¨ë‘ì˜ ë§ë­‰ì¹˜] ê°œì²´ëª… ë¶„ì„ ë§ë­‰ì¹˜ 2021 <br>

# Pretraining ELECTRA-small

<img width="1191" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-12-20 á„‹á…©á„’á…® 12 23 56" src="https://user-images.githubusercontent.com/91872769/209758741-3f2873c7-14eb-423c-8d8e-cb9d008130db.png">

# Result

| Model                     | Macro F1 score |
| :------------------------ | -------------: |
| `KoBERT-NER(Naver NER)`   |           0.34 |
| `KoELECTRA-NER(Finetuned)`|           0.80 |
| `NERSOTA-ELECTRA-NER`     |           0.77 |


# NERSOTA-ELECTRA-small on ğŸ¤— Transformers ğŸ¤—

## 1. Pytorch Model & Tokenizer

```python
from transformers import ElectraModel, ElectraTokenizer

model = ElectraModel.from_pretrained("jieun0115/nersota-electra-small-discriminator")  # KoELECTRA-Small
```

## 2. Tokenizer Example

```python
>>> from transformers import ElectraTokenizer
>>> tokenizer = ElectraTokenizer.from_pretrained("jieun0115/nersota-electra-small-discriminator")
>>> tokenizer.tokenize("[CLS] í•œêµ­ì–´ êµ¬ì–´ì²´ íŠ¹í™” ê°œì²´ëª… ì¸ì‹ê¸°ì…ë‹ˆë‹¤. [SEP]")
['[CLS]', 'í•œêµ­ì–´', 'êµ¬', '##ì–´', '##ì²´', 'íŠ¹í™”', 'ê°œì²´', '##ëª…', 'ì¸ì‹', '##ê¸°', '##ì…ë‹ˆë‹¤', '.', '[SEP]']
>>> tokenizer.convert_tokens_to_ids(['[CLS]', 'í•œêµ­ì–´', 'êµ¬', '##ì–´', '##ì²´', 'íŠ¹í™”', 'ê°œì²´', '##ëª…', 'ì¸ì‹', '##ê¸°', '##ì…ë‹ˆë‹¤', '.', '[SEP]'])
[3, 10751, 1242, 4127, 4385, 19988, 21695, 4101, 7352, 4136, 6896, 1015, 4]
```

# ELECTRA Pretraining

## 1. Requirements

```python
torch==1.12.1
transformers==4.25.1
cudatoolkit == 10.0
sklearn
scipy
```

## 2. Make tfrecords

```bash
# `data` ë””ë ‰í† ë¦¬ë¥¼ ìƒì„± í›„, corpusë¥¼ ì—¬ëŸ¬ ê°œë¡œ ë¶„ë¦¬
$ mkdir data
$ split -a 4 -l {$NUM_LINES_PER_FILE} -d {$CORPUS_FILE} ./data/data_
```

```bash
python3 build_pretraining_dataset.py --corpus-dir data \
                                     --vocab-file vocab.txt \
                                     --output-dir pretrain_tfrecords \
                                     --max-seq-length 128 \
                                     --num-processes 4 \
                                     --no-lower-case
```

## 3. How to Run Pretraining

```bash
# Small model
$ python3 run_pretraining.py --data-dir {$BUCKET_NAME} --model-name {$SMALL_OUTPUT_DIR} --hparams config/small_config.json
```

# ELECTRA Finetuning

## 1. Requirements

```python
torch==1.10.2
transformers==4.18.0
cudatoolkit == 10.2
seqeval
fastprogress
attrdict
```

## 2. How to Run NER

```bash
$ python3 run_ner.py --task ner --config_file koelectra-small.json
```


## Reference
- [ELECTRA](https://github.com/google-research/electra)<br>
- [KoELECTRA](https://github.com/monologg/KoELECTRA)<br>
- [pytorch-bert-crf-ner](https://github.com/eagle705/pytorch-bert-crf-ner)<br>
- [LETR API](https://www.letr.ai/)<br>
- [AIHub](https://www.aihub.or.kr/)<br>
- [ëª¨ë‘ì˜ ë§ë­‰ì¹˜](https://corpus.korean.go.kr/)<br>
- [íŠ¸ìœ„ê·¸íŒœ](https://www.twigfarm.net/)<br>
- [Label Studio](https://labelstud.io/)
