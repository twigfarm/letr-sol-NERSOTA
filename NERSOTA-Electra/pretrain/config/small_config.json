{
    "learning_rate": 5e-4,
    "save_checkpoints_steps": 50000,
    "num_train_steps": 1000000,
    "model_size": "small",
    "vocab_size": 32000,
    "max_seq_length": 128,
    "train_batch_size": 64,
    "embedding_size": 128,
    "generator_hidden_size": 0.25,
    "do_lower_case": false,
    "do_train": false, 
    "do_eval": true,
    "model_hparam_overrides": {
        "max_position_embeddings": 128
    }
}